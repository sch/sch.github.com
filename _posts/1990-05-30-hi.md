---
layout: post
title: Why I Quit Facebook
---

### Adrian Schaedle

- - -

I quit using Facebook earlier this year.

It's very easy to [deactivate your account](http://www.facebook.com/help/account-settings-and-deletion). The link isn't hidden. They do make you feel a little guilty though:


![Are you sure you want to deactivate your account?](https://multimodal.s3.amazonaws.com/deactivate-sure.jpg)
http://www.buzzfeed.com/barbydoc/111-sad-baby-animals-15bc

While deactivated, you don't appear in any list of friends. Your name isn't clickable under pictures you're in. Search for your name on a friend's computer right after you deactivate your account, and you will not be listed.

Your friend might also notice something else more alarming: you've taken them with you. Any picture of them in your photo albums will be gone. Conversations you've held on your profile, groups you've created, anything you've touched, really, is gone for both parties. The second wave of guilt washes over, and suddenly you are King Midas.

Facebook says [they've kept 1 million users a year](http://www.lukew.com/ff/entry.asp?1033) with this tactic. For those who decide to log back in, all posts and pictures and Farmville livestock reappear untouched. You are indexed again. The effect is that of standing at the edge of a tall building focusing at something small down below, and only after stepping back does the relief set in, only because the fear was too large to come at the time.

So to fully delete your Facebook account after this is to admit you are fine with the social suicide, that you are willing to deal with the fallout of 'why' and 'where are those pictures of us'. To delete it (which by the way, you have to [submit a form](http://www.facebook.com/help/contact.php?show_form=delete_account) and enter a four-word captcha and, no shit, wait two weeks without logging back in, accidentally or otherwise, to fully disappear) is admitting that you are okay with how the Facebook world looks deactivated, but without the safety net of being able to log back in at any time. I just really hope I'm making it clear just how gone your stuff is.


## SO WHAT

This is not another essay romanticizing leaving Facebook: those have been written, and they are [pretentious](http://www.themorningnews.org/article/what-i-didnt-write-about-when-i-wrote-about-quitting-facebook) and [glib](http://mashable.com/2011/09/27/you-wont-quit-facebook/). I didn't quit for privacy concerns or addiction, I quit because I didn't like signing my name to stuff that was unimportant, and when I did have something to say, I hated not owning it.

Facebook provides a great affordance to the web: it allows people to find and communicate with each other by name. This is its killer app. You are your name on Facebook, not yourName@anotherWebiste.com, not a contact found by searching on Google, not something conceptually complicated like [OpenID](http://openid.net/). People can search for your name on Facebook and actually find you and add you as a friend, and that's the reason it has been adopted by the millions.

The problem is, Facebook would like to be your identity for the Web at large. It wants to be the one-stop-shop for _you_. The [Open Graph Protocol](http://developers.facebook.com/docs/opengraph/) they've developed is very seductive---a website can merely add hooks to describe something on their site, and Facebook will index not only that object, but when and how you interacted with it. It's an engineering marvel, hashing who you are privately [with cookies](http://nikcub.appspot.com/posts/facebook-fixes-logout-issue-explains-cookies), and it allows for  constant, real-time sharing of whatever the Facebook-semantic web will allow for.

A lot has been said already about the ulterior motives of this monitoring. Part of the Facebook transaction is that you give up your information in return for marvelous, on-the-money ways to interact with your friends, and in turn that information becomes useful for Facebook to sell to marketers and advertisers. Be careful about what you share, and monitor which Facebook apps have access to which data, and you should be okay.

Worry instead how much time and effort you put into Facebook, and any app for that matter. Facebook is a service. You do not own what you put into it, and they provide no way of archiving it. There are ways of talking to Facebook, mostly to put more information into their system, and certainly [some ways](http://socialsafe.net/) of [backing it up](https://www.backupify.com/tour/details/facebook),  but for the most part, Facebook is a destination for your stuff, and it doesn't like to be coaxed out. By making things easier to put into it, there's a weight off of developer's backs and a certain cognitive load off of the user's mind, but the cost is that _you no longer really own your stuff_.

Facebook isn't the only example of this. Applications on the web are made of the same combinations of HTML, CSS, and Javascript that they've been made of for years, but now we find ourselves putting information into similar sites in very abstract ways, and may not have the ability to get it out.

What are the internet's first class citizens? What are key pieces of technology, either baked into web browsers or adopted by convention, that are the permanent fixtures? What are the things, stripped away of their 'social features,' would still be shared around the internet?

- - -

## UNIX AND ONE THING WELL

> _"If you wish to make an apple pie from scratch, you must first invent the universe."_

---Carl Sagan, [Cosmos](http://www.youtube.com/watch?v=7s664NsLeFM)

### A Little History

In the jurassic days of computing, men in white lab coats ran room-sized computers, and generally futzed around trying to figure out what they could be used for. These were not the days of the personal computer: these were the days of a corporation or university owning one expensive mainframe, and entering in instructions by hand. Computers were not connected to one another through the internet, they didn't have suites of word processing and spreadsheet applications, and not even video displays had yet circulated yet; the widespread way of entering commands was through the [ASR-33](http://en.wikipedia.org/wiki/File:ASR_33.jpg) teletype, a "slow, noisy device that printed upper-case-only on big rolls of yellow paper" (TK, http://www.faqs.org/docs/artu/ch02s01.html). There was not a lot of vision at this time for how computers could be used by the world---they were expensive tools to perform research on algorithms and data structures, not connect and share information among a community.

Ken Thompson, a Bell Labs engineer who worked on their Multics time-sharing operating system, wanted to make a game called Space Travel. In 1969, he used a spare PDP-7 computer at Bell to do this.

The Multics OS employed something called **time sharing**, running software in a way that allowed a bunch of users to log in and share time and resources among the others, all on one machine. Multics was a flop however, and the idea of community computing fell by the wayside. Dennis Ritchie, a contributor to the original Unix project and inventor of the C programming language, discussed :

> "We knew from experience that the essence of communal computing, as supplied by remote-access, time-shared machines, is not just to type programs into a terminal instead of a keypunch, but to encourage close communication."

([Ritchie](http://cm.bell-labs.com/cm/cs/who/dmr/hist.html))

Not wanting to repeat the same mistakes made in the bloated Multics environment, he created an entire operating system to write, compile, and run the game. This game earned him some respect for modeling planetary motion, but it was the refined and elegant rewrite of the Multics OS that got him noticed. It would be the basis for Unix, the operating system at the heart of the internet, and the backbone of many computers today.

### Say Something In Unix

Unix operates with **plain text**. You are a user. You access the operating system through a shell prompt:

![An empty shell prompt, waiting for input](https://multimodal.s3.amazonaws.com/unix-blank-prompt.png)

This is where you type commands, press enter, and wait for the computer to process them. There's a command called `cowsay`, and it takes some text as input. So for the following example, we are asking unix to start some arbitrary code callable by the command `cowsay`, and we are giving it some input, namely the word `moo`:

![](https://multimodal.s3.amazonaws.com/unix-cowsay-moo.png)

Commands can also be set to run with certain options called flags. The `cowsay` [manual](http://en.wikipedia.org/wiki/Cowsay) says you can call the `-g` flag to get a greedy cow:

![](https://multimodal.s3.amazonaws.com/unix-cowsay-g.png)

These are cutesy examples, sure. But this abstraction holds some very powerful concepts. A Unix program should be able to be called anywhere in the shell by some command. This command is an alias, and it takes arguments in plain text, has options that can be called, and provides some kind of output in plain text. It's a kind of metaprogramming that did not exist on a system-wide scale before Unix. With Unix, anyone can create a program that works like this. But even more importantly, commands can be chained together with **pipes**.

In Unix programs, plain text goes in, and plain text comes out. The output of one command can be the input of another. For example, the `fortune` program randomly selects a quote to display to the screen. By using a pipe, that output can be the text that is fed into the cowsay program with `fortune | cowsay`:

![](https://multimodal.s3.amazonaws.com/unix-cowsay-fortune.png)

Heavy stuff.

### One Thing Well

What's great about all of this is usually summarized in a pithy quote from one of Unix's founders, Doug McIlroy:

> This is the Unix philosophy: Write programs that do one thing and do it well. Write programs to work together. Write programs to handle text streams, because that is a universal interface.

This quote says a lot about the architecture of Unix, but it says more about the shift that it brought on. More than writing clean, collaborative code, McIlroy's point is that nothing will advance unless you build on the success of others. Unix balanced collaborative computing with time-sharing and pipes, and introduced compatibility and portability of code among linked computers. The rise of Unix ran parallel to the development of ARPANET and the early internet. By making programs small, they were easily maintainable, upgradable, and could be superseded by a better solution when it came along.

> "It seems certain that much of the success of Unix follows from the readability, modifiability, and portability of its software that in turn follows from its expression in high-level languages."

([Ritchie](http://cm.bell-labs.com/cm/cs/who/dmr/hist.html))

### Where It Went

Developer [Andre Torrez](http://twitter.com/#!/torrez/) talks about the early days of developing for the web:

> "There was a joke about getting ideas for making new apps in the late 90’s: just pick a random Unix tool.

> * Talk and IRC begat ICQ, AIM, GTalk, Campfire, Convore.
> * Usenet is at the root of Slashdot, Reddit, Digg, and the multitude of PHPBB communities.
> * [Finger](http://en.wikipedia.org/wiki/Finger_protocol) influenced the creation of weblogs and the idea was further refined as Twitter."

(Torrez, http://notes.torrez.org/2011/12/i-miss-w.html)

It's not really a joke though. The web offers an easier way to link computers together and it facilitates interaction in a more visual way. Those unfamiliar with the Unix Shell, which is powerful, but has a high barrier to entry (learning the file system, the names of programs, working with text and history, decoding its terseness, not offering any safety nets for data, etc), do however understand the visual metaphors of desktop and mobile computing. Within the browser, Google and Wikipedia have taught us to follow links around for discovery, email and Facebook and Twitter have shown how to move text around, Youtube and Flickr tell us where we can upload the stuff we make. The things that Andre Torrez mentions as former Unix tools have all been made: we're now in the era where the paradigm of the browser is known, and it's time to figure out what kind of things we can make next.

- - -

## THE WEB 

Unix was successful above all else because it was a very compatible operating system.

### How It Works

Let's say the web is the internet that can be accessed through a web browsers. Tali Garsiel has written a comprehensive overview of [how modern browsers work](http://www.html5rocks.com/en/tutorials/internals/howbrowserswork/), but for right now, we can say that a browser's job is to draw the page, by parsing and rendering markup.

A web page is written in three parts: HTML, CSS, and Javascript.

#### HTML

HTML is Hypertext Markup Language. Examples of HTML:

* `<p> paragraph text </p>` --- Used for marking text as part of a paragraph. It gives meaning to the text inside of the `<p>` and `</p>` elements, and that meaning is that it is paragraph text.

* `<strong> strong text </strong>` --- This is text that will usually **appear bold** in the browser, but semantically, it is text that is emphasized.

* `<a href="http://www.google.com/"> linked text </a>` --- Text that links to another webpage. The _intent_ of the text is to link, and it holds some information on where to link to: in [this case](http://www.google.com/) to Google's home page.

A [full list](http://simon.html5.org/html-elements) of these elements can be found through the [WHATWG group](http://developers.whatwg.org/) if you're interested.

Most HTML tags are designed to give meaning to the text they surround. For this reason, it's important to mark where these end with `</`element`>` closing '`/`'s. The browser must parse this HTML by reading it nonlinearly, creating a parse tree, by nesting elements within other elements in memory in order to represent their hierarchical structure. This representation is called the Document Object Model or DOM, and at the root of it is the `<html>` tag. Consider this example code from Garciel's paper:

	<html>
		<body>
			<p>
				Hello World
			</p>
			<div> <img src="example.png"/></div>
		</body>
	</html>

This gets turned into the following DOM tree:

![](http://www.html5rocks.com/en/tutorials/internals/howbrowserswork/image015.png)

([Garciel](http://www.html5rocks.com/en/tutorials/internals/howbrowserswork/))

(That `img` tag is one of those tags I was talking about that doesn't describe text, but describes itself---it's an image object, and it includes the url of where that image lives.)

The plain-text world of Unix compared to the richly semantic one of HTML is extremely different. Encoding meaning and structure in the text itself it what allows a browser, a text editor, and applications (consumer or utility) to parse and present and transform that text intelligently. It contains nothing about the presentation of the text: that is the job of CSS and Javascript.

#### CSS

CSS stands for Cascading Style Sheets.

How the page looks belongs to the CSS, or Cascading Style Sheets. It describes how each HTML element should appear in the browser, and it's the middle step between parsing the HTML and rendering 

So for example this bit off CSS:

	p { color: red; }

…would make all the paragraph elements the color red.

Writing CSS is much of the work of designing a web site; the layout, text size, image width, navigation, and color are all things defined by CSS. CSS is mostly static, however. You can describe, say, the color of a link when you hover over it with your mouse, but there's nothing CSS can infer. It's domain-specific---it only acts on elements, the way HTML acts on text. Define a property of an HTML element, and all instances of that element are styled.

There are 245 some odd [CSS attributes](http://www.w3.org/TR/css3-ui/) to modify the presentation of HTML. Notice that there is nothing in CSS that modifies the content of the page: separating the two allows for [the same content](http://www.csszengarden.com/) to be rendered in different ways, allowing many HTML documents to use one style sheet. This makes updating how websites look less cumbersome, but it also means HTML extracted from a web site by another application can be restyled, and that stripped away of all presentation, there will still be a set of defaults to render that HTML. Separating the style means nothing gets lost---and the web must remain backwards compatible.

#### Javascript

Javascript is the programming language of the web. Like HTML, Javascript code is defined with the <code>&lt;script&gt;</code> tag, and executed at render time. It's the same as any other programming language, but it runs in parallel with the browser's rendering engine and interprets clicks and interactions between the user and the site. There's one important thing about Javascript that is different from HTML and CSS, and that is that Javascript can work and change parts of the DOM, turning a static site into something dynamic. Take the following example:

<pre><code>&lt;script&gt;
	document.write("&lt;p&gt;" + Date() + "&lt;/p&gt;");
&lt;/script&gt;
</code></pre>

The script takes the `document` object that represents the DOM, uses the `document` method `write()`, and gives that `write()` method a string so that the date is written as paragraph text.

Entire web pages can be created this way using Javascript, and they often are. Javascript is what powers little drop-down menus on websites, by hiding and showing extra links, drawing them in place literally by adding and subtracting HTML. With it you can have a persistent connection to the web server, polling it for updates, to power, say, a chat program. It's what allows for `<button>` elements to submit something, for little fields where you enter your credit card number to check for validity, and for the autocomplete flood in instant search. Rule of thumb, if something on the page changes, and the entire page didn't refresh, it was the result of Javascript.

#### Other Web Citizens

The web allows for plug-ins, but these are implemented on a browser and operating system basis. Adobe builds [Flash](http://www.adobe.com/products/flashplayer.html), that's used for web video and audio and interactive vector graphics, Microsoft makes [Silverlight](http://www.silverlight.net/) for much of the same (Netflix's instant watch is perhaps the best known use), and things like [Unity](http://unity3d.com/unity/) are used for 3D graphics. These plugins are separate from the material on the web page. They run in their own processes, running code in their own language, and though they appear as an element on the page, they are not a part of the HTML/CSS/Javascript trinity. This can be a good thing: having plugins can dramatically upgrade the capabilities of what you can do on the internet. It can also create a legacy of code that is useless when that plug-in stops being supported. 

The [HTML spec](http://dev.w3.org/html5/spec/Overview.html) is a living, working document, defining useful additions to the web. It's modified to reflect best practices of web development. Video has become an essential part of the web through sites like [Youtube](http://www.youtube.com/) and [Vimeo](http://vimeo.com/). Formerly, presenting video on the web largely required the use of Flash, and when Flash was just something than ran on computers, this was fine. When mobile devices were introduced (see Steve Job's [_Thoughts On Flash_](http://www.apple.com/hotnews/thoughts-on-flash/), support for video became a native application call---a video would be a file played by the operating system, not through a layer of Flash.

The `<video>` element was therefore defined in the HTML spec, transforming video on the web from an abstract plug-in based implementation, to a conceptual and semantic element. It defines a thing, that browsers can implement on their own, and that developers can override visually with CSS and functionally with Javascript, just like any other tag. A `<video>` element can still _use_ Flash to play the clip.

In HTML5, the `<video>` element is joined alongside `<audio>`, `<canvas>` (for bitmapped drawing capabilities), Javascript geolocation calls (for maps and location services), a simple database, and a whole lot of other crazy useful stuff. Mark Pilgrim's [Dive Into HTML5](http://mislav.uniqpath.com/diveintohtml5/index.html) is as good a source as any for this stuff.

Plugins are not the web. What they provide, if it is good, will be folded into the definition and implementation of the web.

#### Anyhow

This separation of content and presentation, of static and dynamic functionality, is what allows the web to be accessible, compatible, and receptive to change. There is always more to worry about though.

## THE APPLICATION WEB

The distinction made between "Web 1.0" and "Web 2.0" is one of interaction. Web 2.0 embraces attributes of New Media objects, making them modular, digital, computer-mediated, interactive, participatory, networked, distributed, and scalable (Stenger, powerpoint). However, these attributes hold for what is considered "Web 1.0" as well. The perceived difference is a shift in power from central distribution of content to user-creation of content on a peer-to-peer basis.

In order to have this peer-to-peer basis, we must still push information around somehow. We are not all expected to host our own webpages, to have the bandwidth for video, or the knowhow for databases. This has given rise to the application-based internet. Javascript got a lot faster, and applications could be created for the first time, because it was the code on the page, not the code on the server way out somewhere on the internet, that was performing the real-time updating.

We can now access our email, have video chats, balance our checkbook, etc, in the browser. This is fine; we wouldn't have these tools any other way. The question is, how are we going to make and interact with these tools? Anyone can make a web app, just like anyone can make a Unix program, and anyone can _use_ the web app, but they do not own it in the same way. They can poke around to see how the browser-side of things works, but there is still a lot of code and database stuff that lives only in the hands of the servers on the original site. You wouldn't want, for example, a bank to hold all of their customer data in the browser when you access it. The browser is a remote control for that content: the real application, the accounts and passwords and piles of cash, lives on the bank's server. 

So how do we let the web talk to other parts of the web? The way to pipe, Unix style, into other apps on the internet, and get back relevant information, is with **API**s.

### APIs

APIs are the Application Program Interfaces. It's a concept that has its roots in object oriented programming---that functionality should be contained within an object, and that any interaction should be a message sent to and received from some request. The result is a functional sandbox: words go in, and words come out. A mystery box that performs addition would have the API that accepts numbers, and spits out a sum. You would never have to see or interact with the code inside: it would just be a specification of how to enter the numbers, and a specification that says how the sum could be returned. Code hiding like this is useful: there's less to go wrong by making the transaction of the code clear.

[Twitter](http://twitter.com/) has very clear [developer documentation](https://dev.twitter.com/docs) and a lot of APIs to work with. Take for example the [**show/:id**](https://dev.twitter.com/docs/api/1/get/statuses/show/%3Aid) method. If you know the unique ID for a URL, you can find out more information about it, like the author, a link to the tweet itself, when it was published. The API for this is clean, it tells you exactly what you must enter, and it tells you exactly what you'll get. It does this with HTTP requests (the same as HTML), not restricting itself to a certain programming language or style.

#### An Example

I'd like to see the information alongside the tweet with the ID `76123424904314880`. So when I enter a HTTP request (you can do this too---just enter  it in your address bar) for the url:

`https://api.twitter.com/1/statuses/show.json?id=76123424904314880&trim_user=1`

Twitter returns a [JSON](http://www.json.org/) key-value pair result:

	{
		"geo":null,
		"truncated":false,
		"created_at":"Thu Jun 02 03:10:13 +0000 2011",
		"place":null,
		"in_reply_to_user_id":null,
		"contributors":null,
		"coordinates":null,
		"user":{
			"id_str":"2735631",
			"id":2735631
		},
		"favorited":false,
		"id_str":"76123424904314880",
		"retweet_count":12,
		"in_reply_to_screen_name":null,
		"in_reply_to_status_id_str":null,
		"source":"\u003Ca href=\"http:\/\/tapbots.com\/tweetbot\" rel=\"nofollow\"\u003ETweetbot for iPhone\u003C\/a\u003E",
		"retweeted":false,
		"in_reply_to_status_id":null,
		"id":76123424904314880,
		"in_reply_to_user_id_str":null,
		"text":"In the 3 years since I'd done yoga, the technology must have really evolved because tonight at class everyone laughed at my yoga machine."
	}

JSON is plain text defining key-value pairs. What is stated as a string of text on the left hand side (the key), has a value (numbers, more strings, a list of more keys, nothing (`null`)) that corresponds with it. It is machine readable because it can be parsed, but it's also human readable: it's easy to see what corresponds to what. If there's a problem, it's easy to debug, because names are well-chosen and because the interface is well-documented. You can use the data returned from the API call to generate HTML, CSS, and Javascript that renders it:

<blockquote class="twitter-tweet tw-align-center"><p>In the 3 years since I'd done yoga, the technology must have really evolved because tonight at class everyone laughed at my yoga machine.</p>&mdash; Adam Lisagor (@lonelysandwich) <a href="https://twitter.com/lonelysandwich/status/76123424904314880" data-datetime="2011-06-02T03:10:13+00:00">June2, 2011</a></blockquote>
<script src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

Striking this balance between simply represented (for the computer's sake) and easily readable (for the geek's sake) is at the heart of the Unix philosophy. Software is useful only when it has a purpose, and maintained only when it can be understood by other programmers. The hundreds of thousands of lines of highly optimized code, written in several different languages, performing tasks at vastly different rates, these are all invisible to people outside the web app. They instead provide just the _services_ of the app; they are unconcerned with the implementation, just the results. They are simple and modular, and they let others make cool stuff.

Twitter is popular among web developers because they have simple tools for authentication. For those that make applications, having an account with a website is a headache. To code it yourself requires a secure database, with the ability to confirm your identity through email, and manage changes to a profile that concern identity, privacy, and history. Plus, persuading new users to sign up for your service is a hard enough task, let alone asking them to remember another username and password. By giving developers a way to ignore this hassle, and gain a list of potential users with follow lists, Twitter turns a mechanic of their service into a feature while promoting Twitter itself. Facebook does the same thing, with friends lists, on steroids.

Though it might seem unlikely, what happens to your data if one of these sites is shut down or discontinued? What if your account is suspended? What happens when the terms of the API change? Depending on an API for functionality on the web is only as strong as your faith in that service. There may be an easy way for parts on the web to communicate, but there's significant danger in relying too heavily on them.

### We Have One Internet, But Many Devices

There's a trend sweeping the web design community right now, and it has been dubbed [Responsive Web Design](http://www.alistapart.com/articles/responsive-web-design). Nothing new has been invented: it's more of a manifesto. The ideas of it start simple: design your site so that the same page looks right no matter where it's being viewed. This is accomplished by scaling down elements and re-arranging them on the screen with CSS, so that everything appears to be at the right size. Sites that are big on a desktop computer will adjust multi-column layouts into single-column ones, they will shrink down giant images to fit the width of the screen, they'll make page navigation and ads and cruft take up exactly the right amount of room. The recent [redesign of The Boston Globe](http://bostonglobe.com/) is a great example of a very mainstream organization using responsive web design to make their presentation better.

It's not all presentation, however. Responsive web design has morphed into something more like _**responsible web design**_, and that's because people have been thinking a lot about the context these sites are viewed in. Someone viewing the Globe on their phone is most likely doing it to find up to the minute news on something big when they are away from a computer, or using it to kill a few minutes waiting for someone or something. Someone viewing that same site on an iPad or tablet is likely doing so at a leisurely pace, in a setting where they're unlikely to do a lot of typing or research. Someone in a web browser on a computer is doing anything in between, using the wide layout of the display.

This isn't even considering the vastly different ways of arriving at the Globe. Some people are regular Globe readers, visiting the site directly, every day. Many don't have any regular allegiance to the Globe, and maybe occasionally receive a link in their email or Twitter, and are taken straight to the article without the context of the rest of the Globe as a whole news corp. Sometimes they never read the actual site, and instead just read it in the stylings of their RSS readers or news aggregators, or just see a blurb from the piece in a blog or a Facebook stream. There are a million ways a site on the internet can be chopped up, transformed, displaced, and displayed.

These are all pretty first-world use cases too---problems you might encounter in modern web browsers, on expensive mobile electronics. People are traditionally slow to upgrade. Up until the last year or two, people were stuck using the super-incompatible Internet Explorer 6, something that Microsoft itself [has been urging people to drop](http://www.ie6countdown.com/). Most people are using older hardware, over internet connections that vary wildly. The idea that what you make will look the same in every browser, even somewhat, is a dream---the closest thing to do is to approximate what you want, and figure out a way for it to degrade elegantly.

These are some of the hallmarks of [web accessibility](http://www.alistapart.com/articles/wiwa/). Making the web accessible doesn't just mean writing HTML semantically in order to for screen readers to help the blind. It means considering contrast for the vision impaired and color blind users, providing transcriptions of audio and video for deaf users, making interaction modal for keyboard/touch input, and considering styling for the screen, phones, print, TV, and more.

### The Application Web Vs. The Application Internet

Responsive and responsible web design is merely a set of guidelines for a good web experience. Ultimately APIs aren't only used for the web, they're used for stand-alone and native applications, and can have real life counterparts. The bank example, anyways, uses ATMs and web sites and phone apps that operate on the same set of data, through APIs.

Mobile devices in the last five years have been the real eye-opening examples of how the internet is not just on the web. The experience of watching a video on Youtube in a desktop browser, vs watching that same video in a Youtube app is a different experience. Even watching a Youtube clip in the web browser of that mobile device is different. Mobile devices, for the sake of simplicity and power management and security (and vendor control), abstract a lot of the functionality of web apps into native apps. Getting linked to a news site article on a mobile device is nice, and then getting asked to consider downloading the news app instead is jarring because it's not immediate and expected. It's jarring also because suddenly the same article exists in two places to the user, with the difference being that the web version is usually unoptimized and plastered with ads, and the app version (which likely has all the same content) must be opened and run, and _shared_ as an application. An app removes a layer of interaction and abstraction (the browser), but raises more questions about how to share what's on the screen. In all likelihood, the app is drawing the same API calls as the browser, but the person using doesn't know that. They must rely on what they know to be the conventions of either medium.

This is not the argument between open and closed platforms. As long as devices have a web browser built into them, they have the ability to get the same experience as the desktop web. Native applications however present a different message to the user. Adopting the new media truism of the medium being the message (Burnett & Marshall, 16), the message of native applications is seductive: if you use a service enough, go through the dedicated, bottom-up experience that has no ads, more video, more tactile methods of interaction, non-zooming layouts, and offline caching. Native apps aren't a URL for a service, they're the service itself, so why use the web app when you can enjoy something built for a specific device?

There's nothing wrong with native apps. They are software, and unlike browser plug-ins, they are tied specifically to a platform and mobile operating system---apps die when the device dies (or in this industry, becomes obsolete). The problem is, what happens when people perceive the internet experience as the web experience? What happens if the app internet supersedes the web app internet?

#### What I Mean: Flickr and Instagram

There are many ways to put pictures on the internet. Facebook is largely for pictures of your friends, [Deviantart](http://www.deviantart.com/) is for drawings, [dump.fm](http://dump.fm/) for animated GIFs, etc, context is everything, the medium is the message, yadda yadda. Photography, and the social parts sharing a small set of artistic photographs, is popular among [Flickr](http://www.flickr.com/) and [Instagram](http://instagram.com/). Flickr has [51 million users](http://advertising.yahoo.com/article/flickr.html) and Instagram has [12 million](http://www.crunchbase.com/company/instagram), and both sites allow for the uploading and sharing of creative photographs. The difference is, Instagram is iPhone only. For pictures that are taken and shared, the entire interaction takes place on the phone. You cannot browse a user's pictures in the browser, you can only receive links to those pictures. They've taken on Flickr with this strategy in less than a year.

### The Black Box Model

This quarantining of information seems so much more felt on mobile devices, because there's no way to scroll up to the top of the page and find a link. The app itself might have some 'share' buttons, but the expectation that there is a link for everything, that that link actually leads to a web page, that the link doesn't have to be merely emailed or tweeted or whatever, has disappeared. It's a tradeoff.

The problem is, the web, the one you access in a browser, is becoming increasingly like this. The modern Javascript application like Facebook or Google+ is useful for connection and navigation, but the content in these web apps can no longer be considered a web page. The ability to link to specific content is what makes the internet the internet. That something can be stated, and permanent, and linked to (and that the link not break when the original site gets updated and the new location become lost) is the real test of a connected internet, and the web app model is not that.

The distinction between a web app and a web page is not a small one. They both may use the same technologies, but they are completely different kinds of interactions.


## THE WAY FORWARD

In terms of manipulating data, the Unix model has become a complete success. Successful web apps (mostly) follow the idea of doing _one thing well_, and the sea of talent, creativity, and cooperation among the people developing these things has increased and flourished. There are more tools doing more useful things for humanity than ever before.

The thing that has become lost is how the information gets passed around. We are long past the days of plain text, and in may ways this is a good thing. HTML holds so much meaning on its own and can be mined for information in complex ways. We do not need to question it. Instead developers and users must ask themselves, how do we push that HTML around?

Certainly, Facebook has been thinking about this. The metadata of a picture or comment is tied to a person, and within Facebook, all of it has structure and an ability to be manipulated freely. Like Unix, they hold information and interaction on a _user based_ plane. Anything created is created by one person. Interactions exist in the context of individuals, I post your wall, you post on mine basis. Photos themselves have links to the people pictured in them. The feed displays the interesting stuff as the algorithm discovers it. How does it play nice with rest of the web though? How can it?

### Anti-social Networking

The thing at the root of any online communication is the user, or some kind of identity. Everything else on the page is plumbing.

Maciej Ceglowski develops a bookmarking site called [Pinboard](http://pinboard.in/), and he argues [there is no social graph](http://blog.pinboard.in/2011/11/the_social_graph_is_neither/). The problems constructing such a graph, where each person is a node and the relationship between those two people is a bidirectional association, are complex technically because a graph as such has no concept of time, and no way to define who sees which nodes without either micromanaging each association or constraining the kinds of relationships between nodes to a predefined set, etc. Ceglowski is more interested if the creation of graph is social at all:

> "There's no way to take a time-out from our social life and describe it to a computer without social consequences… You might almost think that the whole scheme had been cooked up by a bunch of hyperintelligent but hopelessly socially naive people, and you would not be wrong. Asking computer nerds to design social software is a little bit like hiring a Mormon bartender."

([Ceglowski](http://blog.pinboard.in/2011/11/the_social_graph_is_neither/))

The social problem is not necessarily something that can be reverse-engineered. Trying to define our relationship to one another as friends and followers or putting them in circles, all of this is an abstraction of the ridiculous complexity between us. I am somebody's son, brother, friend, enemy, love, acquaintance, creep, and nobody. To me, I am myself, and what is that relationship to you?

The structure of the internet is such that every url is an island. The tools we use to communicate with words and media are increasingly the tools that also present them. What is the universal interface if not the link? Tools like [If This Then That](http://ifttt.com/) crop up every once in a while for services to talk to other services, but there's a tremendous amount of work to be put into making and owning things on the internet. Sharing this stuff is a capital H Hard problem, one that just hasn't been solved yet.

### A Final Thought

Douglas Adams defines the nerd:

> "I think a nerd is a person who uses the telephone to talk to other people about telephones. And a computer nerd therefore is somebody who uses a computer in order to use a computer."

(Adams, [_Triumph of the Nerds_](http://vimeo.com/32078565))

Owning everything you create is not hard, but creating everything yourself in order to own it is.

The web takes great big breaths. There are periods of rapid growth and acceptance, and times when innovation is stagnant. It moves organically, and truly evolves: it's our asset, and it's far too large for any one company to mangage. Compuserve and America Online couldn't own it, Facebook and Google and any other sites with seemingly infinite reach will only be a part of it for as long as they are useful.

It will change because we as humans are the connective tissue. Wherever we will be, however we seek knowledge and communication, the technology will be there to catch up with us. It will seem completely inevitable.

You won't ever have sign out.


- - -

### Thoughts On This Essay

I made a point of making this site myself, and not using a content management system or blogging engine to display it. The finished product is not an essay, but a series of pages on a blog like [Wordpress](http://wordpress.com/).

It's written in Markdown, and that makes the HTML. Images are hosted on Amazon's S3 storage. Heroku serves the web page.

I own it 100%, now what do I do with it?



- - -

- - -

stop reading here

- - -

- - -




<iframe width="600" height="480" src="http://www.youtube.com/embed/OpIYz8tfGjY" frameborder="0" allowfullscreen></iframe>

There's a lot that breaks the fourth wall in 





SHORT ANSWER

Professor Stenger defines some qualities of new media objects, shared across all digital mediums:

modular
digital
computer mediated
interactive
participatory
networked
distributed
scalable


*HUGH KENNER* --- The Elsewhere Community (1998), he observes that the Internet, while it could not replace the direct presence of a teacher, “offers something one-on-one presence does not: access to any mentor who has a computer, anywhere in the world. Media always offers gains, though offset by losses.”

To create something truly new is hard in this business. To make change, you must find a problem that hasn't already been solved. If you're solving a problem in a better way, or fixing something only half-solved, there's more work ahead of you. 

<blockquote class="twitter-tweet tw-align-center"><p>If you're worried about your privacy just end all your Google searches with "I'm asking for a friend."</p>&mdash; Scott Simpson (@scottsimpson) <a href="https://twitter.com/scottsimpson/status/22065112332378112" data-datetime="2011-01-03T23:01:47+00:00">January3, 2011</a></blockquote>
<script src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

<blockquote class="twitter-tweet tw-align-center"><p>The demographic for Foursquare; people who wish they were Sims.</p>&mdash; kelly oxford (@kellyoxford) <a href="https://twitter.com/kellyoxford/status/25545680763" data-datetime="2010-09-26T00:55:55+00:00">September 26, 2010</a></blockquote>
<script src="//platform.twitter.com/widgets.js" charset="utf-8"></script>














## TL;DR
The internet is transient. Only allow yourself to worry about what's on it for a couple of hours a day. Go outside and play. 

kthxbye


ANNOTATED BIBLIOGRAPHY
======================

maciej [Maciej Ceglowski]. “The Social Graph Is Neither”. *Pinboard Blog*. Pinboard Blog, 8 Nov 2011. Web. 9 Nov 2011. <http://blog.pinboard.in/2011/11/the_social_graph_is_neither/>.

**PRIMARY SOURCE**  
Maciej Ceglowski is the creator of Pinboard, an “anti-social bookmarking app”.  As an engineer specializing in archiving and speed in a Delicious-like link store, his view on the current state of social networking is that it is bleak and overdesigned. Ceglowski argues that the current idea of a social graph is too one dimensional, that the nature of friendship and aquaintences is too coarse to define human relationships. He argues it’s impossible to have a detailed and decentralized semantic graph while maintaining privacy over those relationships. The second half of his article is given to taking about how unsocial social networks actually are, and that even if someone solves problem of breadth and privacy, the graph is still incomplete in showing how these relationships evolve over time.

mkruzeniski [Mike Kruzeniski]. “How Print Design is the Future of Interaction”. <http://kruzeniski.com/2011/how-print-design-is-the-future-of-interaction/>

Moss, Trenton. “What Is Web Accessibility?”. *A List Apart: For People Who Make
Websites*. A List Apart Mag., 30 April 2004. Web. 28 November, 2011. <http://www.alistapart.com/articles/wiwa/>

**PRIMARY SOURCE**  
A List Apart is an online publication written by and for people who make websites. While not the first to write about the subject, Trenton Moss introduced the concept of web accessibility to the web-designing masses through the highly influential magazine *A List Apart*. Moss breaks down modern web accessibility into what it was commonly considered: disabililties. This includes blind users with screen readers, augmentation for those with partial sight, contrast for colorblind users, transcipts for deaf users, and other forms of input for those not using a mouse or keyboard. What made this article so influential however was his insight that accessibility means designing semantic and portable content, which is accessed from desktops, handhelds, televisions, printed media, and from slow connections.


Yegge, Steve. “Stevey’s Google Platforms Rant”. *Google+*. Rowan, Rip, 12 October 2011. Web. 19 October 2011. <https://plus.google.com/112678702228711889851/posts/eVeouesvaVX>.

**PRIMARY SOURCE**  
Steve Yegge is a software engineer at Google, formerly a software engineer at Amazon. He accidentally posted an internal memo to his Google+ account publically, detailing engineering and philisophical differences between his two employers, focusing on internal organization and Google’s lack of initiative toward making platforms and protocols (both internal and external) for sharing their technology. His is a frank account of the current state of platform engineering culture among big internet players today, the game of catchup Google must play to compete, and what the nature of web platforms mean for the internet at large.

LINKS
=====
[O’REILLY RADAR](http://radar.oreilly.com/2011/11/sopa-protectip.html) about protect IP

[Google Analytics A Potential Threat to Anonymous Bloggers](http://waxy.org/2011/11/google_analytics/) by Andy Baio

[Pinboard Blog: The Social Graph Is Neither](http://blog.pinboard.in/2011/11/the_social_graph_is_neither/) by maciej

[Why Music ID Resolution Matters](http://notes.variogr.am/post/10733372290/music-resolving-facebook) by Brian Whitman of The Echonest

Steve Yegge’s [internal memo](http://siliconangle.com/furrier/2011/10/12/google-engineer-accidently-shares-his-internal-memo-about-google-platform/) about google+as a platform

GIGAOM: [The rise of the new information gatekeepers](http://gigaom.com/2011/12/01/the-rise-of-the-new-information-gatekeepers/) which is about things like google and siri being arbiters of information adopted by convienence

Ars: [Sites deindexed from google](http://arstechnica.com/tech-policy/news/2011/11/us-judge-orders-hundreds-of-sites-de-indexed-from-google-twitter-bing-facebook.ars)

WIRED: [Bitcoin](http://www.wired.com/magazine/2011/11/mf_bitcoin/all/1) rise and decay

WIRED: [Mexican man decapitated](http://www.wired.com/dangerroom/2011/11/mexican-blogger-decapitated/) for snitching on cartel with social media

THE AWL: [profile](http://www.theawl.com/2011/11/the-man-who-makes-money-publishing-your-nude-pics) of Is Anyone Up

THE AWL: [cannibals on the internet](http://www.theawl.com/2011/03/cannibals-seeking-same-a-visit-to-the-online-world-of-flesh-eaters), meetin’ and eatin’ each other

[How facebook cookies work](http://nikcub.appspot.com/logging-out-of-facebook-is-not-enough)

